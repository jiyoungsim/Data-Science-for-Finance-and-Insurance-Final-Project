{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PS5841 Data Science in Finance and Insurance HW6\n",
    "# Young Sim, js5134"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use X_train, X_test, y_train, y_test for all of the following questions\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv('fraud_data.csv')\n",
    "\n",
    "X = df.iloc[:,:-1]\n",
    "y = df.iloc[:,-1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.016410823768035772"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_one():\n",
    "    \n",
    "    # Your code here\n",
    "    pct_fraud = df.Class.sum()/df.Class.count()    \n",
    "    return  pct_fraud # Return your answer\n",
    "\n",
    "answer_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9852507374631269, 0.0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_two():\n",
    "    from sklearn.dummy import DummyClassifier\n",
    "    from sklearn.metrics import recall_score\n",
    "    \n",
    "    # Your code here\n",
    "    dummy = DummyClassifier(strategy='most_frequent').fit(X_train, y_train)\n",
    "    accuracy_score = dummy.score(X_test, y_test)\n",
    "    y_pre = dummy.predict(X_test)\n",
    "    recall_score_ = recall_score(y_test, y_pre)\n",
    "    return accuracy_score, recall_score_ # Return your answer\n",
    "\n",
    "answer_two()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9907817109144543, 0.375, 1.0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_three():\n",
    "    from sklearn.metrics import recall_score, precision_score\n",
    "    from sklearn.svm import SVC\n",
    "\n",
    "    # Your code here\n",
    "    svc = SVC(gamma = 'auto').fit(X_train, y_train)\n",
    "    y_predict = svc.predict(X_test)\n",
    "    recall_score_ = recall_score(y_test, y_predict)\n",
    "    precision_score_ = precision_score(y_test, y_predict)\n",
    "    accuracy_score = svc.score(X_test, y_test)\n",
    "    return accuracy_score, recall_score_, precision_score_ # Return your answer\n",
    "\n",
    "answer_three()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5320,   24],\n",
       "       [  14,   66]], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_four():\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from sklearn.svm import SVC\n",
    "\n",
    "    # Your code here\n",
    "    svc = SVC(C=1e9, gamma=1e-07).fit(X_train, y_train)\n",
    "    y_score = svc.decision_function(X_test)\n",
    "    y_score = np.where(y_score > -220, 1, 0)\n",
    "    cm = confusion_matrix(y_test, y_score)\n",
    "    return cm # Return your answer\n",
    "\n",
    "answer_four()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.825, 0.9375)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_five():\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.metrics import precision_recall_curve, roc_curve        \n",
    "    lr = LogisticRegression(solver='liblinear').fit(X_train, y_train)\n",
    "    y_score = lr.decision_function(X_test)\n",
    "\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, y_score)\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_score)\n",
    "    \n",
    "    precision_index = np.argwhere(precision==0.75)[0][0]\n",
    "    recall_specified = recall[precision_index]\n",
    "    \n",
    "    fpr_index = np.argwhere(np.round(fpr,2)==0.16)[0][0]\n",
    "    tpr_specified = tpr[fpr_index]\n",
    "    return (recall_specified, tpr_specified) # Return your answer\n",
    "\n",
    "answer_five()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.66666667, 0.76086957],\n",
       "       [0.80072464, 0.80434783],\n",
       "       [0.8115942 , 0.8115942 ],\n",
       "       [0.80797101, 0.8115942 ],\n",
       "       [0.80797101, 0.8115942 ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_six():    \n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "    # Your code here\n",
    "    lr = LogisticRegression(solver='liblinear')\n",
    "    grid_values = {'penalty': ['l1', 'l2'], 'C': [0.01, 0.1, 1, 10, 100]}\n",
    "    grid_lr_recall = GridSearchCV(lr, param_grid=grid_values, scoring='recall', cv = 3)\n",
    "    grid_lr_recall.fit(X_train, y_train)\n",
    "    results = grid_lr_recall.cv_results_\n",
    "    test_scores = np.vstack((results['split0_test_score'], results['split1_test_score'], results['split2_test_score']))\n",
    "    \n",
    "    return test_scores.mean(axis=0).reshape(5, 2) # Return your answer\n",
    "\n",
    "answer_six()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
